{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\sklearn\\lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\sklearn\\qda.py:4: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391, 10)\n",
      "Index(['Date', 'Timestamp', 'Ticker', 'OpenPrice', 'HighPrice', 'LowPrice',\n",
      "       'ClosePrice', 'TotalVolume', 'TotalQuantity', 'TotalTradeCount'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:21: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.ewm(min_periods=2,ignore_na=False,span=3,adjust=True).mean()\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:21: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.ewm(min_periods=9,ignore_na=False,span=10,adjust=True).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End  of array\n",
      "End  of array\n",
      "End  of array\n",
      "End  of array\n",
      "Done\n",
      "391\n",
      "391\n",
      "391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:27: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(center=False,window=30).mean()\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:28: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(center=False,window=30).std()\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:27: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(center=False,window=5).mean()\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:28: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(center=False,window=5).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6880.0\n",
      "8602\n",
      "WARNING:tensorflow:From <ipython-input-1-349077ba5b4e>:181 in train_neural_network.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-1-349077ba5b4e>:184 in train_neural_network.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\tensorflow\\python\\ops\\logging_ops.py:264 in merge_all_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From <ipython-input-1-349077ba5b4e>:189 in train_neural_network.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-349077ba5b4e>:190 in train_neural_network.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "maxAccuracy: 0.410256\n",
      "maxAccuracy: 0.423077\n",
      "maxAccuracy: 0.487179\n",
      "maxAccuracy: 0.5\n",
      "maxAccuracy: 0.512821\n",
      "maxAccuracy: 0.525641\n",
      "maxAccuracy: 0.538462\n",
      "maxAccuracy: 0.551282\n",
      "maxAccuracy: 0.564103\n",
      "maxAccuracy: 0.576923\n",
      "maxAccuracy 0.576923\n"
     ]
    }
   ],
   "source": [
    "# import pandas\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#Chaikin Oscillator  \n",
    "def Chaikin(df):  \n",
    "    ad = (2 * df['ClosePrice'] - df['HighPrice'] - df['LowPrice']) / (df['HighPrice'] - df['LowPrice']) * df['TotalVolume']  \n",
    "    Chaikin = pd.Series(pd.ewma(ad, span = 3, min_periods = 2) - pd.ewma(ad, span = 10, min_periods = 9), name = 'Chaikin')  \n",
    "    df = df.join(Chaikin)  \n",
    "    return df\n",
    "## Bolinger Bands\n",
    "def bbands(price, length=30, numsd=2):\n",
    "    \"\"\" returns average, upper band, and lower band\"\"\"\n",
    "    ave = pd.stats.moments.rolling_mean(price,length)\n",
    "    sd = pd.stats.moments.rolling_std(price,length)\n",
    "    upband = ave + (sd*numsd)\n",
    "    dnband = ave - (sd*numsd)\n",
    "    return np.round(ave,3), np.round(upband,3), np.round(dnband,3)\n",
    "\n",
    "#Create output labels\n",
    "def label(df, numberOfRowsAhead, priceDiffRange):\n",
    "    # Create a list to store the data\n",
    "    labelPosChang = []\n",
    "    labelNegChang = []\n",
    "    labelNoChang = []\n",
    "    for i in range(0, len(df)):\n",
    "       price=df.iloc[i]['ClosePrice']\n",
    "       rowAheadCount=0\n",
    "       if i == len(df)-1:\n",
    "         print(\"End  of array\")  \n",
    "         labelPosChang.append(0)\n",
    "         labelNegChang.append(0)\n",
    "         labelNoChang.append(1)\n",
    "         break \n",
    "          \n",
    "       for j in range(i+1,i+numberOfRowsAhead):\n",
    "        if j > len(df)-1:\n",
    "           print(\"End  of array\")  \n",
    "           labelPosChang.append(0)\n",
    "           labelNegChang.append(0)\n",
    "           labelNoChang.append(1)\n",
    "           break \n",
    "        priceDiff=df.iloc[j]['ClosePrice'] - price\n",
    "         \n",
    "        rowAheadCount = rowAheadCount +1 \n",
    "        if priceDiff > priceDiffRange:\n",
    "           labelPosChang.append(1)\n",
    "           labelNegChang.append(0)\n",
    "           labelNoChang.append(0)\n",
    "           break\n",
    "           \n",
    "        elif priceDiff < -1*priceDiffRange:\n",
    "           labelPosChang.append(0)\n",
    "           labelNegChang.append(1)\n",
    "           labelNoChang.append(0)\n",
    "           break \n",
    "        elif numberOfRowsAhead-1 == rowAheadCount:\n",
    "           labelPosChang.append(0)\n",
    "           labelNegChang.append(0)\n",
    "           labelNoChang.append(1)\n",
    "           break\n",
    "          \n",
    "    print ('Done')\n",
    "    print(len(labelPosChang))\n",
    "    print(len(labelNegChang))\n",
    "    print(len(df))\n",
    "    df['Poschg'] = labelPosChang\n",
    "    df['Negchg'] = labelNegChang\n",
    "    df['Nochg'] = labelNoChang\n",
    "   \n",
    "    return df\n",
    "\n",
    "## Generate data in RNN format\n",
    "def generateRNNInputData(df,previousrows):\n",
    "       X=[]\n",
    "       y=[]\n",
    "       for i in range(previousrows-1, len(df)):\n",
    "            ## to do check whether the right element is selected\n",
    "            #print(df.iloc[i-previousrows+1:i+1][['OpenPrice','HighPrice','LowPrice','TotalVolume','priceChange','movingAverage','upperBB','lowerBB','movingAverage5','upperBB5','lowerBB5','Chaikin']].as_matrix().flatten())\n",
    "            X.append(df.iloc[i-previousrows+1:i+1][['OpenPrice','HighPrice','LowPrice','TotalVolume','priceChange','movingAverage','upperBB','lowerBB','movingAverage5','upperBB5','lowerBB5','Chaikin']].as_matrix().flatten())\n",
    "            y.append(df.iloc[i][['Poschg','Negchg','Nochg']].as_matrix().flatten())\n",
    "            \n",
    "            \n",
    "       #### Data Preprocessing using MinMaxScaler\n",
    "       min_max_scaler = preprocessing.MinMaxScaler()\n",
    "       X = min_max_scaler.fit_transform(X)\n",
    "       return X,y\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "data = pandas.read_csv('C://Users/thame_000/DailyTickSandPNov112017.csv')\n",
    "data=data[data['Ticker']=='MSFT']\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "data=Chaikin(data)\n",
    "data['priceChange'] = data['ClosePrice']- data['ClosePrice'].shift() \n",
    "data['priceChangePred'] = data['ClosePrice'].shift(-1) - data['ClosePrice']\n",
    "\n",
    "data=label(data,5,0.02)\n",
    "\n",
    " \n",
    "          \n",
    "\n",
    "data['movingAverage'], data['upperBB'], data['lowerBB'] = bbands(data.ClosePrice, length=30, numsd=1)\n",
    "data['movingAverage5'], data['upperBB5'], data['lowerBB5'] = bbands(data.ClosePrice, length=5, numsd=1)\n",
    "\n",
    "data.fillna(0, inplace=True)\n",
    "X,y=generateRNNInputData(data,5)\n",
    "\n",
    "input_size=len(X[0])\n",
    "input_size=60\n",
    "\n",
    "global X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "dropout = 0.1 # Dropout, probability to keep units\n",
    "def weight_variable(shape):\n",
    "  #initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  #initial =  tf.random_normal(shape)\n",
    "  return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "\n",
    "                        \n",
    "input_size=60\n",
    "x=tf.placeholder('float',[None,input_size],name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 3],name=\"y_\")\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "dropout = 0.1 # Dropout, probability to keep units\n",
    "n_classes = 3\n",
    "batch_size = 128\n",
    "chunk_size = 12\n",
    "n_chunks = 5\n",
    "rnn_size = 128\n",
    "## Create LSTM cell\n",
    "def lstm_cell_create():\n",
    "  return tf.contrib.rnn.BasicLSTMCell(\n",
    "      size, forget_bias=0.0, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "## Create RNN\n",
    "def recurrent_neural_network(x):\n",
    "    layer = {'weights':tf.Variable(tf.random_normal([rnn_size,n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    #x = tf.transpose(x, [1,0,2])\n",
    "    x = tf.reshape(x, [-1, chunk_size])\n",
    "    x = tf.split(0, n_chunks, x)\n",
    "    with tf.variable_scope('cell_def112'):\n",
    "       lstm_cell = rnn_cell.BasicLSTMCell(rnn_size,state_is_tuple=True)\n",
    "    #lstm_cell=lstm_cell_create()\n",
    "    with tf.variable_scope('rnn_def112'):\n",
    "       outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32 )\n",
    "\n",
    "    output = tf.matmul(outputs[-1],layer['weights']) + layer['biases']\n",
    "\n",
    "    return output\n",
    "## Train neural network\n",
    "def train_neural_network(x, keep_prob,input_size):\n",
    "    prediction=recurrent_neural_network(x)\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y_))\n",
    "    optimizer=tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs=200\n",
    "    \n",
    "    countMin = 0 \n",
    "    minLossEpoch = 1000\n",
    "    dropout=0.1\n",
    "    correct=tf.equal(tf.argmax(prediction,1),tf.argmax(y_,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct,'float'))\n",
    "    maxAccuracy=0;\n",
    "    tf.scalar_summary(\"trainaccuracy\", accuracy)\n",
    "       \n",
    "    # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "        \n",
    "    test_summary=tf.summary.scalar(\"testaccuracy\", accuracy)\n",
    "    logs_path=\"C:/Users/thame_000/tbrnn\"\n",
    "    with tf.Session() as sess:\n",
    "       sess.run(tf.initialize_all_variables())\n",
    "       writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "       for epoch in range(hm_epochs):\n",
    "           epoch_loss=0\n",
    "           batchSize=5\n",
    "           batchStartIndex= 0\n",
    "           batchEndIndex= batchSize\n",
    "           batchCount=0\n",
    "           lengthOfTrainingSet=len(X_train)\n",
    "           #print(\"Length training set\",lengthOfTrainingSet)\n",
    "           while batchEndIndex <= lengthOfTrainingSet:\n",
    "               batchCount=batchCount+1\n",
    "               X_train_batch=X_train[batchStartIndex:batchEndIndex]\n",
    "               y_train_batch=y_train[batchStartIndex:batchEndIndex]\n",
    "               _,c,summary=sess.run([optimizer,cost, summary_op],feed_dict={x:X_train_batch,y_:y_train_batch, keep_prob: dropout})\n",
    "               #print('Epoch',epoch,'completed out of',hm_epochs,'loss_epoch:',c)\n",
    "               valid_acc, valid_summ = sess.run(\n",
    "                            [accuracy, test_summary],\n",
    "                                feed_dict={x:X_test,y_:y_test, keep_prob: dropout})\n",
    "               currAccuracy=valid_acc\n",
    "               writer.add_summary(valid_summ, epoch *  batchSize + batchCount)\n",
    "               #print('Accuracy:',currAccuracy)\n",
    "               if epoch == 0:\n",
    "                  maxAccuracy=currAccuracy\n",
    "               if currAccuracy >  maxAccuracy:\n",
    "                  maxAccuracy=currAccuracy\n",
    "                  print('maxAccuracy:',maxAccuracy)\n",
    "               #if countMin > 100 :\n",
    "                 #break  q\n",
    "               if batchEndIndex >lengthOfTrainingSet-1 :\n",
    "                 break\n",
    "               batchStartIndex=batchEndIndex\n",
    "               batchEndIndex=batchEndIndex+batchSize\n",
    "               if batchEndIndex >lengthOfTrainingSet-1 :\n",
    "                  batchEndIndex=lengthOfTrainingSet\n",
    "               writer.add_summary(summary, epoch *  batchSize + batchCount)\n",
    "       print('maxAccuracy',maxAccuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size=50\n",
    "dropout = 0.1 # Dropout, probability to keep units\n",
    "\n",
    "#12 is number of columns\n",
    "numofinput=12\n",
    "\n",
    "train_neural_network(x, keep_prob,numofinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
