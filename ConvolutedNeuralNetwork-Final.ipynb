{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\sklearn\\lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\sklearn\\qda.py:4: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391, 10)\n",
      "Index(['Date', 'Timestamp', 'Ticker', 'OpenPrice', 'HighPrice', 'LowPrice',\n",
      "       'ClosePrice', 'TotalVolume', 'TotalQuantity', 'TotalTradeCount'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:20: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.ewm(adjust=True,ignore_na=False,min_periods=2,span=3).mean()\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:20: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.ewm(adjust=True,ignore_na=False,min_periods=9,span=10).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End  of array\n",
      "End  of array\n",
      "End  of array\n",
      "End  of array\n",
      "Done\n",
      "391\n",
      "391\n",
      "391\n",
      "<bound method NDFrame.sample of            Date Timestamp Ticker  OpenPrice  HighPrice  LowPrice  ClosePrice  \\\n",
      "70256  20151117  09:31:00   MSFT      53.17      53.52     53.17       53.31   \n",
      "70257  20151117  09:32:00   MSFT      53.32      53.44     53.32       53.38   \n",
      "70258  20151117  09:33:00   MSFT      53.38      53.39     53.14       53.16   \n",
      "70259  20151117  09:34:00   MSFT      53.15      53.25     53.11       53.17   \n",
      "70260  20151117  09:35:00   MSFT      53.17      53.18     53.08       53.08   \n",
      "70261  20151117  09:36:00   MSFT      53.08      53.15     53.05       53.11   \n",
      "70262  20151117  09:37:00   MSFT      53.11      53.18     53.10       53.16   \n",
      "70263  20151117  09:38:00   MSFT      53.16      53.17     53.12       53.17   \n",
      "70264  20151117  09:39:00   MSFT      53.17      53.20     53.14       53.19   \n",
      "70265  20151117  09:40:00   MSFT      53.19      53.24     53.18       53.21   \n",
      "70266  20151117  09:41:00   MSFT      53.21      53.29     53.19       53.26   \n",
      "70267  20151117  09:42:00   MSFT      53.27      53.34     53.27       53.33   \n",
      "70268  20151117  09:43:00   MSFT      53.34      53.38     53.34       53.35   \n",
      "70269  20151117  09:44:00   MSFT      53.36      53.39     53.36       53.37   \n",
      "70270  20151117  09:45:00   MSFT      53.37      53.46     53.36       53.46   \n",
      "70271  20151117  09:46:00   MSFT      53.45      53.49     53.42       53.46   \n",
      "70272  20151117  09:47:00   MSFT      53.45      53.46     53.39       53.41   \n",
      "70273  20151117  09:48:00   MSFT      53.41      53.42     53.32       53.35   \n",
      "70274  20151117  09:49:00   MSFT      53.35      53.39     53.27       53.37   \n",
      "70275  20151117  09:50:00   MSFT      53.37      53.42     53.36       53.39   \n",
      "70276  20151117  09:51:00   MSFT      53.39      53.41     53.35       53.35   \n",
      "70277  20151117  09:52:00   MSFT      53.36      53.40     53.35       53.38   \n",
      "70278  20151117  09:53:00   MSFT      53.38      53.43     53.37       53.40   \n",
      "70279  20151117  09:54:00   MSFT      53.40      53.41     53.37       53.39   \n",
      "70280  20151117  09:55:00   MSFT      53.40      53.40     53.34       53.34   \n",
      "70281  20151117  09:56:00   MSFT      53.34      53.35     53.30       53.33   \n",
      "70282  20151117  09:57:00   MSFT      53.33      53.39     53.32       53.37   \n",
      "70283  20151117  09:58:00   MSFT      53.36      53.43     53.36       53.43   \n",
      "70284  20151117  09:59:00   MSFT      53.43      53.45     53.39       53.43   \n",
      "70285  20151117  10:00:00   MSFT      53.43      53.50     53.42       53.47   \n",
      "...         ...       ...    ...        ...        ...       ...         ...   \n",
      "70617  20151117  15:32:00   MSFT      52.98      52.99     52.96       52.99   \n",
      "70618  20151117  15:33:00   MSFT      52.99      53.08     52.99       53.06   \n",
      "70619  20151117  15:34:00   MSFT      53.05      53.10     53.05       53.06   \n",
      "70620  20151117  15:35:00   MSFT      53.06      53.08     53.06       53.07   \n",
      "70621  20151117  15:36:00   MSFT      53.07      53.10     53.06       53.06   \n",
      "70622  20151117  15:37:00   MSFT      53.06      53.08     53.03       53.06   \n",
      "70623  20151117  15:38:00   MSFT      53.07      53.10     53.06       53.09   \n",
      "70624  20151117  15:39:00   MSFT      53.09      53.09     53.01       53.01   \n",
      "70625  20151117  15:40:00   MSFT      53.02      53.02     52.98       52.99   \n",
      "70626  20151117  15:41:00   MSFT      52.99      53.08     52.99       53.08   \n",
      "70627  20151117  15:42:00   MSFT      53.08      53.08     53.04       53.05   \n",
      "70628  20151117  15:43:00   MSFT      53.04      53.08     53.04       53.08   \n",
      "70629  20151117  15:44:00   MSFT      53.08      53.12     53.07       53.10   \n",
      "70630  20151117  15:45:00   MSFT      53.10      53.11     53.08       53.08   \n",
      "70631  20151117  15:46:00   MSFT      53.08      53.09     53.04       53.04   \n",
      "70632  20151117  15:47:00   MSFT      53.05      53.05     53.01       53.01   \n",
      "70633  20151117  15:48:00   MSFT      53.01      53.02     52.95       52.95   \n",
      "70634  20151117  15:49:00   MSFT      52.95      52.97     52.93       52.95   \n",
      "70635  20151117  15:50:00   MSFT      52.95      52.99     52.92       52.92   \n",
      "70636  20151117  15:51:00   MSFT      52.92      53.04     52.92       53.02   \n",
      "70637  20151117  15:52:00   MSFT      53.02      53.03     52.95       52.96   \n",
      "70638  20151117  15:53:00   MSFT      52.97      53.00     52.96       52.97   \n",
      "70639  20151117  15:54:00   MSFT      52.97      53.03     52.97       53.02   \n",
      "70640  20151117  15:55:00   MSFT      53.03      53.03     52.99       53.00   \n",
      "70641  20151117  15:56:00   MSFT      52.99      53.00     52.94       52.97   \n",
      "70642  20151117  15:57:00   MSFT      52.96      52.97     52.92       52.93   \n",
      "70643  20151117  15:58:00   MSFT      52.93      53.00     52.92       52.97   \n",
      "70644  20151117  15:59:00   MSFT      52.98      53.00     52.97       52.98   \n",
      "70645  20151117  16:00:00   MSFT      52.98      53.00     52.93       52.98   \n",
      "70646  20151117  16:01:00   MSFT      52.97      52.97     52.97       52.97   \n",
      "\n",
      "       TotalVolume  TotalQuantity  TotalTradeCount    ...     priceChangePred  \\\n",
      "70256   6182527.49         115881              638    ...                0.07   \n",
      "70257   6821707.72         127801              786    ...               -0.22   \n",
      "70258  11418055.93         214281              972    ...                0.01   \n",
      "70259   6856201.12         128926              679    ...               -0.09   \n",
      "70260   7174853.56         135094              720    ...                0.03   \n",
      "70261   7082665.85         133386              714    ...                0.05   \n",
      "70262   5769976.01         108563              625    ...                0.01   \n",
      "70263   5375770.50         101151              426    ...                0.02   \n",
      "70264   3734820.67          70235              339    ...                0.02   \n",
      "70265   4930792.26          92675              512    ...                0.05   \n",
      "70266   5368281.24         100836              421    ...                0.07   \n",
      "70267   4618662.56          86643              466    ...                0.02   \n",
      "70268   4606064.51          86327              573    ...                0.02   \n",
      "70269   2841199.08          53232              316    ...                0.09   \n",
      "70270   7449959.17         139471              662    ...                0.00   \n",
      "70271   4389787.51          82120              461    ...               -0.05   \n",
      "70272   6212333.22         116285              565    ...               -0.06   \n",
      "70273   5630030.63         105482              584    ...                0.02   \n",
      "70274   8258767.06         154873              787    ...                0.02   \n",
      "70275   2841105.10          53214              345    ...               -0.04   \n",
      "70276   3736938.27          69999              467    ...                0.03   \n",
      "70277   3036961.37          56903              390    ...                0.02   \n",
      "70278   1825658.47          34184              235    ...               -0.01   \n",
      "70279   2000575.76          37472              265    ...               -0.05   \n",
      "70280   3390523.93          63519              293    ...               -0.01   \n",
      "70281   3286742.63          61632              381    ...                0.04   \n",
      "70282   1969310.85          36906              232    ...                0.06   \n",
      "70283   3458613.68          64790              378    ...                0.00   \n",
      "70284   2860237.94          53540              341    ...                0.04   \n",
      "70285   4854017.98          90825              498    ...                0.04   \n",
      "...            ...            ...              ...    ...                 ...   \n",
      "70617   3626410.73          68455              443    ...                0.07   \n",
      "70618   4212267.83          79429              471    ...                0.00   \n",
      "70619   4112940.82          77515              426    ...                0.01   \n",
      "70620   1751340.74          33000              174    ...               -0.01   \n",
      "70621   4465750.04          84135              514    ...                0.00   \n",
      "70622   5476745.79         103226              601    ...                0.03   \n",
      "70623   2010256.34          37874              236    ...               -0.08   \n",
      "70624   4405476.37          83034              453    ...               -0.02   \n",
      "70625   3490898.53          65862              339    ...                0.09   \n",
      "70626   5450501.45         102788              628    ...               -0.03   \n",
      "70627   3463513.35          65281              397    ...                0.03   \n",
      "70628   4801259.52          90495              537    ...                0.02   \n",
      "70629   7654933.88         144154              784    ...               -0.02   \n",
      "70630   4257357.83          80193              430    ...               -0.04   \n",
      "70631   5038791.80          94943              552    ...               -0.03   \n",
      "70632   4082746.00          76988              420    ...               -0.06   \n",
      "70633   5578095.30         105288              554    ...                0.00   \n",
      "70634   7037137.89         132901              628    ...               -0.03   \n",
      "70635   5967924.32         112692              541    ...                0.10   \n",
      "70636   8821847.03         166450              934    ...               -0.06   \n",
      "70637   5295890.74          99957              556    ...                0.01   \n",
      "70638   7306249.96         137916              778    ...                0.05   \n",
      "70639   7100300.84         133957              760    ...               -0.02   \n",
      "70640   5035092.19          94987              525    ...               -0.03   \n",
      "70641   6548639.27         123629              703    ...               -0.04   \n",
      "70642   7624635.78         144020              831    ...                0.04   \n",
      "70643   5770539.70         108949              693    ...                0.01   \n",
      "70644   8010459.09         151189              814    ...                0.00   \n",
      "70645  20077516.72         379031             1332    ...               -0.01   \n",
      "70646    148051.15           2795                1    ...                0.00   \n",
      "\n",
      "       Poschg  Negchg  Nochg  movingAverage  upperBB  lowerBB  movingAverage5  \\\n",
      "70256       1       0      0          0.000    0.000    0.000           0.000   \n",
      "70257       0       1      0          0.000    0.000    0.000           0.000   \n",
      "70258       0       1      0          0.000    0.000    0.000           0.000   \n",
      "70259       0       1      0          0.000    0.000    0.000           0.000   \n",
      "70260       1       0      0          0.000    0.000    0.000          53.220   \n",
      "70261       1       0      0          0.000    0.000    0.000          53.180   \n",
      "70262       1       0      0          0.000    0.000    0.000          53.136   \n",
      "70263       1       0      0          0.000    0.000    0.000          53.138   \n",
      "70264       1       0      0          0.000    0.000    0.000          53.142   \n",
      "70265       1       0      0          0.000    0.000    0.000          53.168   \n",
      "70266       1       0      0          0.000    0.000    0.000          53.198   \n",
      "70267       1       0      0          0.000    0.000    0.000          53.232   \n",
      "70268       1       0      0          0.000    0.000    0.000          53.268   \n",
      "70269       1       0      0          0.000    0.000    0.000          53.304   \n",
      "70270       0       1      0          0.000    0.000    0.000          53.354   \n",
      "70271       0       1      0          0.000    0.000    0.000          53.394   \n",
      "70272       0       1      0          0.000    0.000    0.000          53.410   \n",
      "70273       1       0      0          0.000    0.000    0.000          53.410   \n",
      "70274       1       0      0          0.000    0.000    0.000          53.410   \n",
      "70275       0       1      0          0.000    0.000    0.000          53.396   \n",
      "70276       1       0      0          0.000    0.000    0.000          53.374   \n",
      "70277       0       1      0          0.000    0.000    0.000          53.368   \n",
      "70278       0       1      0          0.000    0.000    0.000          53.378   \n",
      "70279       0       1      0          0.000    0.000    0.000          53.382   \n",
      "70280       1       0      0          0.000    0.000    0.000          53.372   \n",
      "70281       1       0      0          0.000    0.000    0.000          53.368   \n",
      "70282       1       0      0          0.000    0.000    0.000          53.366   \n",
      "70283       1       0      0          0.000    0.000    0.000          53.372   \n",
      "70284       1       0      0          0.000    0.000    0.000          53.380   \n",
      "70285       1       0      0         53.319   53.431   53.208          53.406   \n",
      "...       ...     ...    ...            ...      ...      ...             ...   \n",
      "70617       1       0      0         52.964   53.011   52.917          53.008   \n",
      "70618       0       0      1         52.966   53.015   52.917          53.020   \n",
      "70619       1       0      0         52.968   53.019   52.916          53.028   \n",
      "70620       1       0      0         52.971   53.026   52.916          53.032   \n",
      "70621       1       0      0         52.973   53.030   52.916          53.048   \n",
      "70622       1       0      0         52.974   53.033   52.916          53.062   \n",
      "70623       0       1      0         52.977   53.039   52.915          53.068   \n",
      "70624       1       0      0         52.977   53.039   52.915          53.058   \n",
      "70625       1       0      0         52.978   53.040   52.917          53.042   \n",
      "70626       0       1      0         52.985   53.047   52.923          53.046   \n",
      "70627       1       0      0         52.991   53.049   52.933          53.044   \n",
      "70628       1       0      0         52.997   53.055   52.938          53.042   \n",
      "70629       0       1      0         53.002   53.062   52.942          53.060   \n",
      "70630       0       1      0         53.008   53.067   52.950          53.078   \n",
      "70631       0       1      0         53.012   53.069   52.955          53.070   \n",
      "70632       0       1      0         53.015   53.069   52.960          53.062   \n",
      "70633       0       1      0         53.016   53.069   52.963          53.036   \n",
      "70634       0       1      0         53.016   53.068   52.964          53.006   \n",
      "70635       1       0      0         53.016   53.068   52.964          52.974   \n",
      "70636       0       1      0         53.016   53.068   52.964          52.970   \n",
      "70637       1       0      0         53.016   53.068   52.964          52.960   \n",
      "70638       1       0      0         53.017   53.068   52.965          52.964   \n",
      "70639       0       1      0         53.018   53.069   52.967          52.978   \n",
      "70640       0       1      0         53.019   53.070   52.969          52.994   \n",
      "70641       0       1      0         53.021   53.069   52.972          52.984   \n",
      "70642       1       0      0         53.019   53.070   52.968          52.978   \n",
      "70643       0       0      1         53.018   53.069   52.967          52.978   \n",
      "70644       0       0      1         53.017   53.068   52.965          52.970   \n",
      "70645       0       0      1         53.014   53.066   52.963          52.966   \n",
      "70646       0       0      1         53.014   53.066   52.962          52.966   \n",
      "\n",
      "       upperBB5  lowerBB5  \n",
      "70256     0.000     0.000  \n",
      "70257     0.000     0.000  \n",
      "70258     0.000     0.000  \n",
      "70259     0.000     0.000  \n",
      "70260    53.342    53.098  \n",
      "70261    53.298    53.062  \n",
      "70262    53.175    53.097  \n",
      "70263    53.179    53.097  \n",
      "70264    53.187    53.097  \n",
      "70265    53.206    53.130  \n",
      "70266    53.238    53.158  \n",
      "70267    53.296    53.168  \n",
      "70268    53.339    53.197  \n",
      "70269    53.371    53.237  \n",
      "70270    53.426    53.282  \n",
      "70271    53.456    53.332  \n",
      "70272    53.460    53.360  \n",
      "70273    53.460    53.360  \n",
      "70274    53.460    53.360  \n",
      "70275    53.438    53.354  \n",
      "70276    53.400    53.348  \n",
      "70277    53.386    53.350  \n",
      "70278    53.397    53.359  \n",
      "70279    53.401    53.363  \n",
      "70280    53.398    53.346  \n",
      "70281    53.399    53.337  \n",
      "70282    53.396    53.336  \n",
      "70283    53.412    53.332  \n",
      "70284    53.428    53.332  \n",
      "70285    53.461    53.351  \n",
      "...         ...       ...  \n",
      "70617    53.036    52.980  \n",
      "70618    53.055    52.985  \n",
      "70619    53.068    52.988  \n",
      "70620    53.075    52.989  \n",
      "70621    53.081    53.015  \n",
      "70622    53.066    53.058  \n",
      "70623    53.081    53.055  \n",
      "70624    53.087    53.029  \n",
      "70625    53.083    53.001  \n",
      "70626    53.090    53.002  \n",
      "70627    53.087    53.001  \n",
      "70628    53.083    53.001  \n",
      "70629    53.103    53.017  \n",
      "70630    53.096    53.060  \n",
      "70631    53.094    53.046  \n",
      "70632    53.098    53.026  \n",
      "70633    53.095    52.977  \n",
      "70634    53.063    52.949  \n",
      "70635    53.023    52.925  \n",
      "70636    53.013    52.927  \n",
      "70637    52.997    52.923  \n",
      "70638    53.000    52.928  \n",
      "70639    53.021    52.935  \n",
      "70640    53.022    52.966  \n",
      "70641    53.009    52.959  \n",
      "70642    53.012    52.944  \n",
      "70643    53.012    52.944  \n",
      "70644    52.995    52.945  \n",
      "70645    52.987    52.945  \n",
      "70646    52.987    52.945  \n",
      "\n",
      "[391 rows x 22 columns]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:26: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=30,center=False).mean()\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:27: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=30,center=False).std()\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:26: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=5,center=False).mean()\n",
      "C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\ipykernel\\__main__.py:27: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=5,center=False).std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X[0]\n",
      "[]\n",
      "numberofcolumns\n",
      "12\n",
      "WARNING:tensorflow:From <ipython-input-1-065d05b65721>:211 in train_neural_network.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "WARNING:tensorflow:From <ipython-input-1-065d05b65721>:222 in train_neural_network.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-065d05b65721>:223 in train_neural_network.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-1-065d05b65721>:226 in train_neural_network.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From C:\\Users\\thame_000\\Anaconda3new\\lib\\site-packages\\tensorflow\\python\\ops\\logging_ops.py:264 in merge_all_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "maxAccuracy: 0.551282\n",
      "maxAccuracy: 0.666667\n",
      "maxAccuracy: 0.692308\n"
     ]
    }
   ],
   "source": [
    "# import pandas\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#Chaikin Oscillator  \n",
    "def Chaikin(df):  \n",
    "    ad = (2 * df['ClosePrice'] - df['HighPrice'] - df['LowPrice']) / (df['HighPrice'] - df['LowPrice']) * df['TotalVolume']  \n",
    "    Chaikin = pd.Series(pd.ewma(ad, span = 3, min_periods = 2) - pd.ewma(ad, span = 10, min_periods = 9), name = 'Chaikin')  \n",
    "    df = df.join(Chaikin)  \n",
    "    return df\n",
    "## Bolinger Bands\n",
    "def bbands(price, length=30, numsd=2):\n",
    "    \"\"\" returns average, upper band, and lower band\"\"\"\n",
    "    ave = pd.stats.moments.rolling_mean(price,length)\n",
    "    sd = pd.stats.moments.rolling_std(price,length)\n",
    "    upband = ave + (sd*numsd)\n",
    "    dnband = ave - (sd*numsd)\n",
    "    return np.round(ave,3), np.round(upband,3), np.round(dnband,3)\n",
    "\n",
    "#Create output labels\n",
    "def label(df, numberOfRowsAhead, priceDiffRange):\n",
    "    # Create a list to store the data\n",
    "    labelPosChang = []\n",
    "    labelNegChang = []\n",
    "    labelNoChang = []\n",
    "    for i in range(0, len(df)):\n",
    "       price=df.iloc[i]['ClosePrice']\n",
    "       rowAheadCount=0\n",
    "       if i == len(df)-1:\n",
    "         print(\"End  of array\")  \n",
    "         labelPosChang.append(0)\n",
    "         labelNegChang.append(0)\n",
    "         labelNoChang.append(1)\n",
    "         break \n",
    "          \n",
    "       for j in range(i+1,i+numberOfRowsAhead):\n",
    "        if j > len(df)-1:\n",
    "           print(\"End  of array\")  \n",
    "           labelPosChang.append(0)\n",
    "           labelNegChang.append(0)\n",
    "           labelNoChang.append(1)\n",
    "           break \n",
    "        priceDiff=df.iloc[j]['ClosePrice'] - price\n",
    "         \n",
    "        rowAheadCount = rowAheadCount +1 \n",
    "        if priceDiff > priceDiffRange:\n",
    "           labelPosChang.append(1)\n",
    "           labelNegChang.append(0)\n",
    "           labelNoChang.append(0)\n",
    "           break\n",
    "           \n",
    "        elif priceDiff < -1*priceDiffRange:\n",
    "           labelPosChang.append(0)\n",
    "           labelNegChang.append(1)\n",
    "           labelNoChang.append(0)\n",
    "           break \n",
    "        elif numberOfRowsAhead-1 == rowAheadCount:\n",
    "           labelPosChang.append(0)\n",
    "           labelNegChang.append(0)\n",
    "           labelNoChang.append(1)\n",
    "           break\n",
    "          \n",
    "    print ('Done')\n",
    "    print(len(labelPosChang))\n",
    "    print(len(labelNegChang))\n",
    "    print(len(df))\n",
    "    df['Poschg'] = labelPosChang\n",
    "    df['Negchg'] = labelNegChang\n",
    "    df['Nochg'] = labelNoChang\n",
    "   \n",
    "    return df\n",
    "\n",
    "## Generate CNN data using historical data \n",
    "def generateCNNInputData(df,previousrows):\n",
    "       X=[]\n",
    "       y=[]\n",
    "       for i in range(previousrows-1, len(df)):\n",
    "            ## to do check whether the right element is selected\n",
    "            #print(df.iloc[i-previousrows+1:i+1][['OpenPrice','HighPrice','LowPrice','TotalVolume','priceChange','movingAverage','upperBB','lowerBB','movingAverage5','upperBB5','lowerBB5','Chaikin']].as_matrix().flatten())\n",
    "            X.append(df.iloc[i-previousrows+1:i+1][['OpenPrice','HighPrice','LowPrice','TotalVolume','priceChange','movingAverage','upperBB','lowerBB','movingAverage5','upperBB5','lowerBB5','Chaikin']].as_matrix().flatten())\n",
    "            y.append(df.iloc[i][['Poschg','Negchg','Nochg']].as_matrix().flatten())\n",
    "            \n",
    "            \n",
    "       print(\"(X[0]\")\n",
    "       print(X[:0])\n",
    "       #### Data Preprocessing using MinMaxScaler\n",
    "       min_max_scaler = preprocessing.MinMaxScaler()\n",
    "       X = min_max_scaler.fit_transform(X)\n",
    "       return X,y\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pandas.read_csv('C://Users/thame_000/DailyTickSandPNov112017.csv')\n",
    "data=data[data['Ticker']=='MSFT']\n",
    "\n",
    "data=Chaikin(data)\n",
    "data['priceChange'] = data['ClosePrice']- data['ClosePrice'].shift() \n",
    "data['priceChangePred'] = data['ClosePrice'].shift(-1) - data['ClosePrice']\n",
    "\n",
    "data=label(data,5,0.02)\n",
    "\n",
    " \n",
    "          \n",
    "\n",
    "\n",
    "data['movingAverage'], data['upperBB'], data['lowerBB'] = bbands(data.ClosePrice, length=30, numsd=1)\n",
    "data['movingAverage5'], data['upperBB5'], data['lowerBB5'] = bbands(data.ClosePrice, length=5, numsd=1)\n",
    "\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "print(data.sample)\n",
    "            \n",
    "X,y=generateCNNInputData(data,5)\n",
    "\n",
    "input_size=len(X[0])\n",
    "input_size=60\n",
    "\n",
    "### Split into trai and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "dropout = 0.9 # Dropout, probability to keep units\n",
    "def weight_variable(shape):\n",
    "  #initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  #initial =  tf.random_normal(shape)\n",
    "  return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def bias_variable(shape):\n",
    "  #initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        \n",
    "input_size=60\n",
    "x=tf.placeholder('float',[None,input_size],name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 3],name=\"y_\")\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "dropout = 0.8 # Dropout, probability to keep units\n",
    "\n",
    "## Create CNN Network\n",
    "def neural_network_model(x,keep_prob,numberofcolumns ):\n",
    "        numberofbackrecord=5\n",
    "        numberoffilters1=32\n",
    "        numberoffilters2=64\n",
    "        print(\"numberofcolumns\")\n",
    "        print(numberofcolumns)\n",
    "              \n",
    "        W_conv1 = weight_variable([1, numberofbackrecord, 1, numberoffilters1])\n",
    "        b_conv1 = bias_variable([numberoffilters1])\n",
    "        x_image = tf.reshape(x, [-1,numberofcolumns,numberofbackrecord,1])\n",
    "        \n",
    "        \n",
    "        ## conv 1 and max layer 1 \n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1=h_conv1\n",
    "        \n",
    "        W_conv2 = weight_variable([1, numberofbackrecord, numberoffilters1, numberoffilters2])\n",
    "        b_conv2 = bias_variable([numberoffilters2])\n",
    "\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2=h_conv2\n",
    "        \n",
    "        ## dense layer1\n",
    "        W_fc1 = weight_variable([numberofcolumns * numberofbackrecord * numberoffilters2, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "\n",
    "        \n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, numberofcolumns * numberofbackrecord * numberoffilters2])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        \n",
    "         ## drop layer\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        W_fc2 = weight_variable([1024, 3])\n",
    "        b_fc2 = bias_variable([3])\n",
    "\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        \n",
    "      \n",
    "        return y_conv\n",
    "\n",
    "def train_neural_network(x, keep_prob,input_size):\n",
    "    prediction=neural_network_model(x, keep_prob,input_size)\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y_))\n",
    "    #learning rate defaulted\n",
    "    optimizer=tf.train.AdamOptimizer().minimize(cost)\n",
    "    logs_path=\"C:/Users/thame_000/tbcnn\"\n",
    "    writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "    hm_epochs=200\n",
    "    \n",
    "    countMin = 0 \n",
    "    minLossEpoch = 1000\n",
    "    dropout=0.9\n",
    "    correct=tf.equal(tf.argmax(prediction,1),tf.argmax(y_,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct,'float'))\n",
    "    maxAccuracy=0;\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "       sess.run(tf.initialize_all_variables())\n",
    "       tf.scalar_summary(\"trainaccuracy\", accuracy)\n",
    "       \n",
    "       # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "       summary_op = tf.merge_all_summaries()\n",
    "        \n",
    "       test_summary=tf.summary.scalar(\"testaccuracy\", accuracy)\n",
    "    \n",
    "       for epoch in range(hm_epochs):\n",
    "           epoch_loss=0\n",
    "           batchSize=5\n",
    "           batchStartIndex= 0\n",
    "           batchEndIndex= batchSize\n",
    "           batchCount=0\n",
    "           \n",
    "           lengthOfTrainingSet=len(X_train)\n",
    "           #print(\"Length training set\",lengthOfTrainingSet)\n",
    "           while batchEndIndex <= lengthOfTrainingSet:\n",
    "               batchCount=batchCount+1\n",
    "               X_train_batch=X_train[batchStartIndex:batchEndIndex]\n",
    "               y_train_batch=y_train[batchStartIndex:batchEndIndex]\n",
    "               _,c,summary=sess.run([optimizer,cost, summary_op],feed_dict={x:X_train_batch,y_:y_train_batch, keep_prob: dropout})\n",
    "               #print('Epoch',epoch,'completed out of',hm_epochs,'loss_epoch:',c)\n",
    "               valid_acc, valid_summ = sess.run(\n",
    "                            [accuracy, test_summary],\n",
    "                                feed_dict={x:X_test,y_:y_test, keep_prob: dropout})\n",
    "               writer.add_summary(valid_summ, epoch *  batchSize + batchCount)\n",
    "               currAccuracy=valid_acc\n",
    "               #print('Accuracy:',currAccuracy)\n",
    "               if epoch == 0:\n",
    "                  maxAccuracy=currAccuracy\n",
    "               if currAccuracy >  maxAccuracy:\n",
    "                  maxAccuracy=currAccuracy\n",
    "                  print('maxAccuracy:',maxAccuracy)\n",
    "               #if countMin > 100 :\n",
    "                 #break  q\n",
    "               if batchEndIndex >lengthOfTrainingSet-1 :\n",
    "                 break\n",
    "               batchStartIndex=batchEndIndex\n",
    "               batchEndIndex=batchEndIndex+batchSize\n",
    "               if batchEndIndex >lengthOfTrainingSet-1 :\n",
    "                  batchEndIndex=lengthOfTrainingSet\n",
    "                # write log\n",
    "               writer.add_summary(summary, epoch *  batchSize + batchCount)\n",
    "            \n",
    "       print('maxAccuracy',maxAccuracy)\n",
    "       \n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#12 is number of columns\n",
    "numofinput=12\n",
    "\n",
    "train_neural_network(x, keep_prob,numofinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
